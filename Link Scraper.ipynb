{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\"\"\"\n",
    "Created on Tue Dec 29 14:05:26 2020\n",
    "\n",
    "@author: bturkkani\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "links = []\n",
    "headers = {\"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; WOW64; rv:54.0) Gecko/20100101 Firefox/84.0\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "key = input(\"key phrase (type 'done' when done): \")\n",
    "pdat = input(\"pages to scrape (type 'std' after number if constant e.g '1 std'): \")\n",
    "while key != 'done':\n",
    "    pdat.split()\n",
    "    pmax = (int(pdat.split()[0])-1)*10\n",
    "    page = 0\n",
    "    while page <= pmax:\n",
    "        key = key.replace(' ', '+')\n",
    "        url = 'https://www.google.com/search?q={}&start={}'.format(key,page)\n",
    "        resp = requests.get(url, headers=headers)\n",
    "        if resp.status_code == 200:\n",
    "            soup = BeautifulSoup(resp.content, \"html.parser\")\n",
    "            for g in soup.find_all('div', class_='g'):\n",
    "                    rc = g.find('div', class_='rc')\n",
    "                    if rc:\n",
    "                        divs = rc.find_all('div', recursive=False)\n",
    "                        if len(divs) >= 2:\n",
    "                            anchor = divs[0].find('a')\n",
    "                            link = anchor['href']\n",
    "                            title = anchor.find('h3').text\n",
    "                            item = {\"title\": title,\"link\": link}\n",
    "                            links.append(item)\n",
    "            page = page + 10\n",
    "        else:\n",
    "            if resp.status_code == 429: print('Google blocked you. Try again tomorrow.')\n",
    "            else: print('Error ' + str(resp.status_code) + ': page not found or smt look it up')\n",
    "            break\n",
    "    key = input(\"key phrase (type 'done' when done): \")\n",
    "    if key != 'done':\n",
    "        if pdat.split()[-1] != 'std':\n",
    "            pdat = input(\"pages to scrape (type 'std' after number if constant e.g '1 std'): \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(links)\n",
    "df.drop_duplicates()\n",
    "df.to_excel('links.xlsx')\n",
    "df.to_csv('links.csv')\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
